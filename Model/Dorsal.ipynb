{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-bc5f4f62a54b>:7: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for GPU\n",
    "import tensorflow as tf\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "# else:\n",
    "#     print('Please install GPU version of TF')\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "#check keras and tf versions\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dicts from csv\n",
    "##------------------------------ Dataset -------------------------------------##\n",
    "import csv\n",
    "import threading\n",
    "\n",
    "\n",
    "#csv.field_size_limit(10**7)\n",
    "\n",
    "def read_dict(path):\n",
    "    'Reads Python dictionary stored in a csv file'\n",
    "    dictionary = {}\n",
    "    for key, val in csv.reader(open(path)):\n",
    "        dictionary[key] = val\n",
    "    return dictionary\n",
    "\n",
    "# Load partitions\n",
    "partition = read_dict('/autofs/space/euler_001/users/lstm/dicts/partition.csv')\n",
    "\n",
    "def read_patterns(csv_file):\n",
    "    '''\n",
    "    Reads in target patterns of the form:\n",
    "        label,v1,v2,...,vN\n",
    "    '''\n",
    "    pattern_dict = {}\n",
    "    with open(csv_file,'r') as f:\n",
    "        for line in f:\n",
    "            atoms = line.strip().split(',')\n",
    "            word = atoms[0]\n",
    "            pattern = np.array([float(x) for x in atoms[1:]])\n",
    "            pattern_dict[word] = pattern\n",
    "    return pattern_dict\n",
    "\n",
    "# Load dictionary of labels\n",
    "labels = read_patterns('/autofs/space/euler_001/users/lstm/dicts/DorsalSparse.csv')\n",
    "#print(labels)\n",
    "\n",
    "\n",
    "exec(\"partition['train'] = \" + partition['train'])\n",
    "exec(\"partition['validation'] = \" + partition['validation'])\n",
    "exec(\"partition['test'] = \" + partition['test'])\n",
    "\n",
    "# # Final computations\n",
    "# partition['train'] = partition['train'] \n",
    "# partition['validation'] = partition['validation']\n",
    "# partition['test'] = partition['test']\n",
    "\n",
    "partition['train'] = partition['train'] + partition['validation']\n",
    "partition['validation'] = partition['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Data Generator\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    my_list = [] #global variable for getting the IDs\n",
    "    my_labels = [] #global variable for getting the labels\n",
    "    def __init__(self, list_IDs, labels, batch_size=100, dim=(max_len,211), #n_channels=1,\n",
    "                 n_classes=300, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        #self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.my_list = []\n",
    "        self.my_labels = []\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # FA and NS added this part to get the names of the words\n",
    "        if index == 0:\n",
    "            indexes2 = self.indexes[index*800:(index+1)*800]\n",
    "            list_IDs_temp2 = [self.list_IDs[k] for k in indexes2]\n",
    "            self.my_list = list_IDs_temp2 # This is for getting shuffled list_IDs for get_my_list\n",
    "            \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))#, self.n_channels))\n",
    "        #y = np.empty((self.batch_size), dtype=int)\n",
    "        y = np.empty((self.batch_size, max_len, self.n_classes), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('Cochs/' + ID + '.npy') #path to cochleagrams stored in npy format\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, LSTM, Masking, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.metrics import categorical_crossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (max_len,211),\n",
    "          'batch_size': 100,\n",
    "          'n_classes': 300,\n",
    "          #'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "partition = partition\n",
    "labels = labels\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params) #9 token per word\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params) #1 token per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 226, 211)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 226, 512)          1482752   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 226, 300)          153900    \n",
      "=================================================================\n",
      "Total params: 1,636,652\n",
      "Trainable params: 1,636,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_dim = 211\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-9999, input_shape=(max_len, data_dim))) #The mask value has to equal whatever the junk value was that you used to pad the inputs. 0. is a bad idea because it might actually occur in the cgram. In EARSHOT we use -9999.\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dense(300, activation='sigmoid')) \n",
    "#bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)#, clipnorm=0.01)\n",
    "#sgd=SGD(lr=1e-3, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mse',\n",
    "              optimizer=opt)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load the model from checkpoint\n",
    "# model = tf.keras.models.load_model('/autofs/space/euler_001/users/lstm/DorsalModel_CheckPoints/Mid1334/_1334.hd5f')\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping and checkpoint\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "# earlystop = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50),\n",
    "#              ModelCheckpoint(filepath='VentralModel_EarlyStop.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "# checkpoint\n",
    "checkpoints = ModelCheckpoint('checkpoints/' + '_{epoch:02d}' + '.hd5f',\n",
    "                              monitor=\"loss\",\n",
    "                              verbose=0,\n",
    "                              #save_best_only=True,\n",
    "                              save_weights_only=False,\n",
    "                              mode=\"min\",\n",
    "                              save_freq=7900)\n",
    "\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "terminate = TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Time-based decay\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# initial_learning_rate = 0.0001\n",
    "# epochs = 10000\n",
    "# decay = initial_learning_rate / epochs\n",
    "\n",
    "# def lr_time_based_decay(epoch, lr):\n",
    "#     return lr * 1 / (1 + decay * epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Step Decay\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# # Define the learning rate schedule function\n",
    "# initial_learning_rate = 0.01\n",
    "# epochs = 10000\n",
    "\n",
    "# def lr_step_decay(epoch, lr):\n",
    "#     drop_rate = 0.0001\n",
    "#     epochs_drop = 10.0\n",
    "#     return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Exponential Decay\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# initial_learning_rate = 0.01\n",
    "# epochs = 10000\n",
    "# def lr_exp_decay(epoch, lr):\n",
    "#     k = 0.001\n",
    "#     return initial_learning_rate * math.exp(-k*epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we train the Network.\n",
    "#history = model.fit(X_train, Y_train, batch_size=100, epochs=1000, validation_data=(X_valid, Y_valid))\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                              epochs=10000,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoints],# LearningRateScheduler(lr_step_decay, verbose=1)], #earlystop,\n",
    "                              #use_multiprocessing=True,\n",
    "                              workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot loss\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim([0, 1])\n",
    "plt.legend(loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
