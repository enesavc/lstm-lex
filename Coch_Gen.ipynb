{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-bc5f4f62a54b>:7: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for GPU\n",
    "import tensorflow as tf\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "# else:\n",
    "#     print('Please install GPU version of TF')\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "#check keras and tf versions\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cochleagram Generation\n",
    "\n",
    "Loads an audio file and generates a cochleagram using the tfcochleagram.py library. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# For displaying audio and images in notebook\n",
    "#import IPython.display as ipd\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isdir, join, dirname, join, realpath\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import tfcochleagram\n",
    "import h5py\n",
    "import math\n",
    "# Helper functions for loading audio\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the 4d padding function adapted from Kevin's 2d version\n",
    "def pad(data_4d, ref_shape, pad_val=-9999):\n",
    "    '''\n",
    "    Pads a 4D matrix \"data\" with the pad_value to make it have shape ref_shape.\n",
    "    All the padded values occur \"off the end\" of the data array, so that the\n",
    "    original array occupies the upper left of the resulting matrix.\n",
    "    Refer to pad_nans method to pad with nan values\n",
    "    '''\n",
    "    padded_data = pad_val*np.ones(ref_shape)\n",
    "    padded_data[:, :data_4d.shape[1], :data_4d.shape[2], :] = data_4d\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "#Here we calculate the longest time and store it into max len\n",
    "t0 = time.time()\n",
    "cwd = os.getcwd()\n",
    "PATH = join(cwd, 'Words')\n",
    "\n",
    "onlydirs = [f for f in listdir(PATH) if isdir(join(PATH, f))]\n",
    "\n",
    "total_len = []\n",
    "\n",
    "for dirs in onlydirs:\n",
    "    dirname = PATH + \"/\" + dirs \n",
    "    rfnArraymax = [os.path.join(dirname, f)for f in os.listdir(dirname) if f.endswith('.wav')]\n",
    "    \n",
    "    for f in rfnArraymax:\n",
    "        test_audio, SR = load_audio_wav_resample(f, DUR_SECS='full', resample_SR=16000)\n",
    "        total_len.append(math.ceil(len(test_audio)/80))\n",
    "        #print(math.ceil(len(test_audio)/80))\n",
    "max_len = np.max(total_len)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3852.8852260112762\n"
     ]
    }
   ],
   "source": [
    "#Write cochs to individual np arrays\n",
    "j = 0\n",
    "for dirs in onlydirs:\n",
    "    dirname = PATH + \"/\" + dirs \n",
    "    rfnArray = [os.path.join(dirname, f)for f in os.listdir(dirname) if f.endswith('.wav')]\n",
    "    i = 0\n",
    "    \n",
    "    for f in rfnArray:\n",
    "        j = j + 1\n",
    "        print(j, end='\\r')\n",
    "        #print('Running demo with sound file: %s ' % f)\n",
    "        test_audio, SR = load_audio_wav_resample(f, DUR_SECS='full', resample_SR=20000)\n",
    "        # Generally a good idea to rms normalize the audio\n",
    "        test_audio = rms_normalize_audio(test_audio, rms_value=0.01)\n",
    "        # Using rFFT below, and it is currently implemented only for even # signals. \n",
    "        if len(test_audio.ravel())%2:\n",
    "            test_audio = test_audio[:-1]\n",
    "            #print(test_audio)\n",
    "        if len(test_audio.shape) == 1: # we need to make sure the input node has a first dimension that corresponds to the batch size\n",
    "            test_audio = np.expand_dims(test_audio,0) \n",
    "        nets = {}\n",
    "        # tfcochleagram expects a dictionary with 'input_signal' defined for the input audio\n",
    "        nets['input_signal'] = tf.Variable(test_audio, dtype=tf.float32)\n",
    "        nets = tfcochleagram.cochleagram_graph(nets, SR, rFFT=True)\n",
    "        #with tf.Session() as sess:\n",
    "        #with tf.compat.v1.Session() as sess:\n",
    "        nets['input_signal'] = test_audio\n",
    "        cochleagram = nets['cochleagram']\n",
    "        filters_out = nets['filts_tensor']\n",
    "        \n",
    "        #this is where padding happening\n",
    "        cochleagram = pad(cochleagram, (1,211,max_len,1), -9999) #211 is the channels in the coch.\n",
    "        cochleagram = cochleagram[:,:,:,0] #here we get rid of the last axis because we do not need depth\n",
    "        cochleagram = np.rollaxis(cochleagram,1,3) # here we move the second axis (1) 3 times to make it totalXtimeXchannels\n",
    "        #print(cochleagram.shape)\n",
    "        \n",
    "        #save the cochs into pickles\n",
    "        MAIN_PATH = join(cwd, 'data')\n",
    "        #if isdir(MAIN_PATH + '/%s' %dirs) == False:  #for now we do not want to create subfolders    \n",
    "        # os.mkdir(MAIN_PATH + '/%s' %dirs)\n",
    "        \n",
    "        filenames_with_extension = os.listdir(dirname)\n",
    "        \n",
    "        filenames=[x.split('.')[0] for x in filenames_with_extension]\n",
    "        filename=filenames[i]\n",
    "        i += 1\n",
    "        #filex=os.path.basename(filename)       \n",
    "\n",
    "        #write to numpy array\n",
    "        #f = MAIN_PATH + '/%s' %dirs + '/' + filename #no subfolers under data folder\n",
    "        f = MAIN_PATH + '/' + filename\n",
    "        np.save(f,cochleagram[0,:,:])\n",
    "        #write to pickle\n",
    "        #import pickle\n",
    "        #with open(MAIN_PATH + '/%s' %dirs + '/' + filename +'.PICKLE', 'wb') as f:\n",
    "        #    pickle.dump(cochleagram[:,:,:], f, protocol= 4)\n",
    "        \n",
    "t1 = time.time()\n",
    "timer = t1-t0\n",
    "print(timer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
